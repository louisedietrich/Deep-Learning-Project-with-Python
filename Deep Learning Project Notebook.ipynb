{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [00:04<00:00, 74.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [00:03<00:00, 81.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 825.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 718.89it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 772.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 767.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crazings: 300\n",
      "Inclusions: 300\n",
      "Patches: 300\n",
      "Pitted Surface: 300\n",
      "Rolled In Scale: 300\n",
      "Scratches: 300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create classes for the image processing of the steel defects \n",
    "class SteelDefects():\n",
    "    IMG_SIZE = 200\n",
    "\n",
    "    Crazing = 'Database/Crazing'\n",
    "    Inclusions = 'Database/Inclusions'\n",
    "    Patches = 'Database/Patches'\n",
    "    PittedSurface = 'Database/PittedSurface'\n",
    "    RolledInScale = 'Database/RolledInScale'\n",
    "    Scratches = 'Database/Scratches'\n",
    "    #We give class values to the images from the different files\n",
    "    LABELS = {Crazing : 0 , Inclusions : 1 , Patches : 2, PittedSurface : 3, RolledInScale : 4, Scratches : 5}\n",
    "    #Creating an empty list, training_data, that will be later filled with the images and their labels\n",
    "    training_data = []\n",
    "    \n",
    "    #Setting counters to 0 (to append training samples to training data)\n",
    "    CrazingCount = 0\n",
    "    InclusionsCount = 0\n",
    "    PatchesCount = 0\n",
    "    PittedSurfaceCount = 0\n",
    "    RolledInScaleCount = 0\n",
    "    ScratchesCount = 0\n",
    "    \n",
    "    #Creating the data we will train on by associating images to their right class/label\n",
    "    def associate_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            #print(label)\n",
    "            for file in tqdm(os.listdir(label)):\n",
    "                path = os.path.join(label, file) \n",
    "                img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) #read images in grey scale \n",
    "                #Resizing of the images as defined before: 200*200\n",
    "                img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                self.training_data.append([np.array(img), np.eye(6)[self.LABELS[label]]])\n",
    "                #We want to have a one hot vector format with our labels to index images \n",
    "                #We used np.eye(6) which is an 6x6 identity matrix (corresponding to our 6 different labels)\n",
    "            \n",
    "                \n",
    "                if label == self.Crazing:\n",
    "                            self.CrazingCount += 1\n",
    "                elif label == self.Inclusions:\n",
    "                            self.InclusionsCount += 1\n",
    "                elif label == self.Patches:\n",
    "                            self.PatchesCount += 1\n",
    "                elif label == self.PittedSurface:\n",
    "                            self.PittedSurfaceCount += 1\n",
    "                elif label == self.RolledInScale:\n",
    "                            self.RolledInScaleCount += 1\n",
    "                elif label == self.Scratches:\n",
    "                            self.ScratchesCount += 1\n",
    "    \n",
    "        \n",
    "        #Shuffling the data that will fill the training_data list\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print('Crazings:',self.CrazingCount)\n",
    "        print('Inclusions:',self.InclusionsCount)\n",
    "        print('Patches:',self.PatchesCount)\n",
    "        print('Pitted Surface:',self.PittedSurfaceCount)\n",
    "        print('Rolled In Scale:',self.RolledInScaleCount)\n",
    "        print('Scratches:',self.ScratchesCount)\n",
    "\n",
    "steeldefects = SteelDefects()\n",
    "steeldefects.associate_training_data()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a perfect balance between classes (300 images per label). It is a necessary condition for our neural network model, which will then not be induced to choose an unbalanced category during its training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "#After setting REBUIlD_DATA to False, we store the training_data as an array in a npy file\n",
    "training_data = np.load(\"training_data.npy\", allow_pickle = True) \n",
    "#Lenght of our training_data\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sample has 1800 images (300*6 labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[132, 120, 119, ..., 161, 157, 148],\n",
      "       [126, 128, 126, ..., 178, 177, 163],\n",
      "       [139, 133, 125, ..., 157, 161, 154],\n",
      "       ...,\n",
      "       [142, 144, 170, ..., 154, 159, 148],\n",
      "       [159, 141, 153, ..., 159, 167, 169],\n",
      "       [153, 156, 151, ..., 159, 148, 151]], dtype=uint8)\n",
      " array([0., 0., 0., 0., 1., 0.])]\n"
     ]
    }
   ],
   "source": [
    "#printing the pixel matrix and the class of the first image of our sample\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one hot vector is pointing out the class of the first image of the sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=4608, out_features=576, bias=True)\n",
      "  (fc2): Linear(in_features=576, out_features=320, bias=True)\n",
      "  (fc3): Linear(in_features=320, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "#Building of the model\n",
    "#We create a class, called Model, that will inherit methods from the nn.Module class\n",
    "    def __init__(self):\n",
    "        super().__init__() #The super() function returns an object that represents the parent class\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3) \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3) \n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "\n",
    "        #Creating random data (x), pass it through CNN and reshaping it the right size for passing through the  linear neural network \n",
    "        x = torch.randn(200,200).view(-1,1,200,200)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "      \n",
    "       # Now calling the initialization of the fully connected (fc) neurons network\n",
    "        self.fc1 = nn.Linear(self._to_linear, 576) \n",
    "        self.fc2 = nn.Linear(576, 320) \n",
    "        self.fc3 = nn.Linear(320, 6) #output size of 6 as we have 6 differents classes of defects\n",
    "\n",
    "#Defining the forward method ONLY for the CNN    \n",
    "    def convs(self, x): \n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (3, 3)) #pooling with a 3*3 window\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (3, 3))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (3, 3))\n",
    "\n",
    "        #Making sure that the data passing trough the CNN has the right size before the fc network\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "#Now defining the entire forward method for the WHOLE model (CNN and linear network)\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x) #passing through the convolutional layers\n",
    "        x = x.view(-1, self._to_linear)  #shaping the data to be flattened\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "#Adam Optimizer for a stochastic optimization (learning rate 1e-3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating xs and ys from our data (xs are pixel values, y is the one hot vector)\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,200,200)\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540\n"
     ]
    }
   ],
   "source": [
    "Val_percent = 0.3 # we save 30% of our data for testing\n",
    "val_size = int(len(X)*Val_percent)\n",
    "print(val_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our validation sample size is 540 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1260\n",
      "540\n"
     ]
    }
   ],
   "source": [
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are training our model on 1260 images and, as seen before, testing it on 540 pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:52<00:00,  2.82s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.1562793254852295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:51<00:00,  2.79s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Loss: 0.11358287930488586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:55<00:00,  2.89s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2. Loss: 0.07440149784088135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:55<00:00,  2.90s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3. Loss: 0.06312646716833115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:52<00:00,  2.81s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4. Loss: 0.031805042177438736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:55<00:00,  2.88s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5. Loss: 0.0333588570356369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:57<00:00,  2.94s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6. Loss: 0.025439513847231865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:57<00:00,  2.93s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7. Loss: 0.032403185963630676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:59<00:00,  2.99s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8. Loss: 0.031079314649105072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:58<00:00,  2.97s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9. Loss: 0.028474221006035805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:57<00:00,  2.95s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10. Loss: 0.02782069705426693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:58<00:00,  2.95s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11. Loss: 0.02620222605764866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:57<00:00,  2.93s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12. Loss: 0.03357209265232086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:56<00:00,  2.92s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13. Loss: 0.030377903953194618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [02:20<00:00,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14. Loss: 0.02795458771288395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#batch size = 32 and epochs = 15 for training\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15 #number of times we pass data through the model to evaluate it\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, len(train_X), BATCH_SIZE)): #From 0, with steps = 32, for the lenght of train_X(=1260)\n",
    "        batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 200, 200)\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "    \n",
    "        #Setting gradients to 0 before the loss calculation\n",
    "        optimizer.zero_grad()   \n",
    "        outputs = model(batch_X)\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()    #Optimizer adjust the weights to lower the loss over time (learning rate of 1e-3)\n",
    "\n",
    "    print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 540/540 [00:48<00:00, 11.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  96.3 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing the accuracy of the model's prediction\n",
    "correct = 0\n",
    "total = 0\n",
    "#Just evaluating our model accuracy without calculating gradients: using torch.no_grad()\n",
    "with torch.no_grad(): \n",
    "    for i in tqdm(range(len(test_X))):\n",
    "        real_class = torch.argmax(test_y[i])\n",
    "        model_out = model(test_X[i].view(-1, 1, 200, 200))[0] \n",
    "        predicted_class = torch.argmax(model_out)\n",
    "\n",
    "        if predicted_class == real_class:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "print(\"Accuracy: \", round(correct/total*100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This neural network gives us a 96.3% accuracy in predicting the right classes of the steel defects for the 540 tested pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
